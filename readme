This is the submission for the Conversational AI : Data Science Lab evaluation by Dhruv Goel (roll no-102083029)
I have been assigned the Cartoon Classification task from Kaggle(link: https://www.kaggle.com/volkandl/cartoon-classification ) 
------------------------------------------------------------------------------------------------------------------
ABOUT THE DATASET:

Data set contains 10 different famous cartoon pictures gathered from the mix videos as frames. The aim behind that is to predict which frame belongs which cartoon.
In other words, can we classify cartoons according to their drawing style and the features (signatures) behind them.

There are 10k training images and 2k test images associated with each class. The images are high resolution with the size 250X350
------------------------------------------------------------------------------------------------------------------
APPROACH:
Data Preprocessing- I have implemented data preprocessing using imagedatagen iterators. This includes rescaling,shear_range,zoom_range and vertical flipping
I have tried two approaches to solve this problem:
1) TRANSFER LEARNING: I used the VGG19 CNN architecture as a pretrained model. VGG19 is trained on imagenet dataset which has 1000 classes. First we freeze the weights of the convolutional blocks and only train 2 fully connected layer's weights then we have 10 dense softmax layers for classification into 10 classes. I ran the model for 50 epochs, the training accurary reaached to about 92%, however the validation accuracy and loss kept fluctuating meaning the model was overfitting. I could have used early stopping but the validation accuracy would have been around 83%. The total training time on GPU took about 6 hours. Due to time contraint of the submission, I decided to try another approach. link-https://www.kaggle.com/dhruvgoel200113/vgg19-implementation
                      
2) CNN (DEEP) : I built a 10 layer deep convolutional neural network consisting of 4 blocks. The first block  was a 2D convolution layer with relu activation. The 2nd 3rd and 4th block were 2 2D convolution layers with relu activation followed by a maxpooling layer with a dropout=0.2. The final layer was a globalpooling layer followed by 10 dense layers for classification of 10 classes. The train time for 1 epoch without the GPU crossed 160 hours, so I trained the model on the kaggle GPU accelerator which reduced the running time of epoch to approx 1 hour. I also implemented kernel regularization to control overfitting. The training accuracy and validation accuracy peaked by the second epoch. Training accuracy was 98+% and the validation accuracy was 85%. link- https://www.kaggle.com/dhruvgoel200113/cnn-implementation?scriptVersionId=90225588
 
    
